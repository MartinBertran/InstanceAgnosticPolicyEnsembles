import torch
import collections
import numpy as np


DetraceFromLogpReturns = collections.namedtuple(
    'DetraceFromLogpReturns',
    ['vs', 'pg_advantages', 'log_rhos',
     'rollout_action_log_probs', 'target_action_log_probs'])

DetraceReturns = collections.namedtuple('DetraceReturns', ['vs', 'pg_advantages'])

def from_logp(
        rollout_action_log_probs, target_action_log_probs, dones_tp1,
        reward_tp1, values_t, bootstrap_values, n=1, gamma_value = 0.999, pg_mode ='n_step',
        clip_threshold_u=None, clip_threshold_d=None,
        clip_traj_threshold_u=None, clip_traj_threshold_d=None
        ):
    '''
    Importance sampling for softmax policies.

    Calculates  actor critic targets for softmax polices

    :param rollout_action_log_probs: float32 tensor of shape [T,B,???]  log probability of behaviour policy (log b(a_t|s_t))
    :param target_action_log_probs: float32 tensor of shape [T,B,???]  log probability of target policy (log \pi(a_t|s_t))
    :param clip_threshold: float 32 clipping threshold for probability ratios
    :param values_t: float32 tensor of shape [T,B,???] with value function estimates wrt the target policy (V(s_t))
    :param reward_tp1:  float32 tensor of shape [T,B,???]  with the rewards generated by following the behaviour policy. (r_{t+1})
    :param dones_tp1: bool tensor of shape [T,B,???]  indicating if current episode is terminated (d_{t+1})
    :param rho: float32 tensor of shape [T,B,???]  likelihood ratio of target and behaviour policy for executed action  ( \rho_t = pi(a_t|s_t)/b(a_t|s_t))
    :param bootstrap_values: float32 tensor of shape [B,???] value estimates at time T+1 used to close unended episodes (V(s_{T+1})
    :param n: int number of steps before bootstrapping
    :param gamma_vlaue: float discount factor
    :param pg_mode: str one-step or n-step return for pg advantages

    :return: A `DetraceFromLogpReturns` namedtuple with the following fields:
        vs: A float32 tensor of shape [T, B]. Can be used as target to train a
            baseline (V(x_t) - vs_t)^2.
        pg_advantages: A float 32 tensor of shape [T, B]. Can be used as an
          estimate of the advantage in the calculation of policy gradients.
        log_rhos: A float32 tensor of shape [T, B] containing the log importance
          sampling weights (log rhos).
        behaviour_action_log_probs: A float32 tensor of shape [T, B] containing
          behaviour policy action log probabilities (log \mu(a_t)).
        target_action_log_probs: A float32 tensor of shape [T, B] containing
          target policy action probabilities (log \pi(a_t)).
    '''
    with torch.no_grad():
        log_rhos = target_action_log_probs - rollout_action_log_probs
        # clipped_rhos =torch.exp(log_rhos)

        # if clip_threshold_u is not None:
        #     clip_threshold_u = torch.tensor(clip_threshold_u,dtype=torch.float32)
        #     clipped_rhos = torch.min(clip_threshold_u, clipped_rhos)
        # if clip_threshold_d is not None:
        #     clip_threshold_d = torch.tensor(clip_threshold_d,dtype=torch.float32)
        #     clipped_rhos = torch.max(clip_threshold_d, clipped_rhos)

        clipped_log_rhos =log_rhos

        if clip_threshold_u is not None:
            clip_log_threshold_u = torch.log(torch.tensor(clip_threshold_u,dtype=torch.float32))
            clipped_log_rhos = torch.min(clip_log_threshold_u, clipped_log_rhos)
        if clip_threshold_d is not None:
            clip_log_threshold_d = torch.log(torch.tensor(clip_threshold_d,dtype=torch.float32))
            clipped_log_rhos = torch.max(clip_log_threshold_d, clipped_log_rhos)

        detrace_returns =importance_sampling(
            values_t=values_t,
            reward_tp1=reward_tp1,
            dones_tp1=dones_tp1,
            # rho=clipped_rhos,
            log_rho=clipped_log_rhos,
            bootstrap_values =bootstrap_values,
            n=n,
            gamma_value=gamma_value,
            pg_mode=pg_mode,
            clip_traj_threshold_u=clip_traj_threshold_u,
            clip_traj_threshold_d=clip_traj_threshold_d

        )
        return DetraceFromLogpReturns(
            log_rhos=log_rhos,
            rollout_action_log_probs=rollout_action_log_probs,
            target_action_log_probs=target_action_log_probs,
            **detrace_returns._asdict()
        )



def importance_sampling(values_t, reward_tp1, dones_tp1, rho=None,
                        log_rho=None, bootstrap_values =None, n=1,
                        gamma_value=0.9,pg_mode='one_step',
                        clip_traj_threshold_u=None,
                        clip_traj_threshold_d=None
                        ):
    '''
    :param values_t: float32 tensor of shape [T,B,???] with value function estimates wrt the target policy (V(s_t))
    :param reward_tp1:  float32 tensor of shape [T,B,???]  with the rewards generated by following the behaviour policy. (r_{t+1})
    :param dones_tp1: bool tensor of shape [T,B,???]  indicating if current episode is terminated (d_{t+1})
    :param rho: float32 tensor of shape [T,B,???]  likelihood ratio of target and behaviour policy for executed action  ( \rho_t = pi(a_t|s_t)/b(a_t|s_t))
    :param bootstrap_values: float32 tensor of shape [B,???] value estimates at time T+1 used to close unended episodes (V(s_{T+1})
    :param n: int number of steps before bootstrapping
    :param gamma_vlaue: float discount factor
    :param pg_mode: str one-step or n-step return for pg advantages
    :return:  value_targets_array [T,B,???] value targets for critic loss (v_t) , policy_gradient_advs

    Update equation:
    g(s_t) = \sum_{i=1}^n \gamma^{i-1} \Pi_{j=0}^{i-1} \rho_{t+j} r_{t+i} + \gamma^n \Pi_{j=0}^{n-1} V^{\pi}(s_{t+n})
    pg_t = (r_{t+1) + \gamma V^{\pi}(s_{t+1}) - V^{\pi}(s_{t})) P(s_0 -> s_t, r_t | pi) / P(s_0 -> s_t, r_t | b)
    P(s_0 -> s_t, r_t | pi) / P(s_0 -> s_t, r_t | b) = \Pi_0^t \rho_t (resetting between episodes)
    '''

    if log_rho is None:
        log_rho = torch.zeros_like(reward_tp1)

    if clip_traj_threshold_u is not None:
        clip_log_threshold_u = torch.log(torch.tensor(clip_traj_threshold_u, dtype=torch.float32))
    else:
        clip_log_threshold_u = torch.tensor(float('inf'))
    if clip_traj_threshold_d is not None:
        clip_log_threshold_d = torch.log(torch.tensor(clip_traj_threshold_d, dtype=torch.float32))
    else:
        clip_log_threshold_d = - torch.tensor(float('inf'))

    if bootstrap_values is None:
        bootstrap_values = torch.zeros_like(reward_tp1)
    ones_like = torch.ones([*dones_tp1.shape[1:]])
    zeros_like = torch.zeros([*dones_tp1.shape[1:]])
    values_tp1 = torch.cat([values_t[1:], bootstrap_values.unsqueeze(0)], dim=0)

    #policy gradient importance weight
    previous_log_cum_rhos = torch.zeros([*dones_tp1.shape[1:]])
    cumulative_log_rhos = torch.zeros_like(values_t)

    # gamma update matrix
    log_gamma_array = np.zeros([n, *dones_tp1.shape[1:]]) + np.log(gamma_value)
    log_gamma_array[0] = 0
    log_gamma_array = np.array(np.cumsum(log_gamma_array, axis=0)[::-1])
    log_gamma_array = torch.tensor(log_gamma_array, dtype=torch.float32)

    # log_rho update matrix
    log_rho_window = torch.zeros_like(log_gamma_array)

    # influence matrix
    influence_window = torch.zeros_like(log_gamma_array)
    influence_window[-1] = 1
    influence_reset = torch.zeros_like(log_gamma_array)
    influence_reset[-1] = 1


    # value targets
    g_value_targets = torch.zeros_like(values_t)

    # current episode onset
    not_done = torch.ones([*dones_tp1.shape[1:]])
    for i in range(dones_tp1.shape[0]):
        # update episode length and "previous update" variables
        prev_not_done = not_done
        not_done = (1 - dones_tp1[i, ...]).float()  # not done
        i_low = max(0,i-n+1) # lower bound of effective influence window
        w_low = max(-n,-i-1) # length of effective influence window

        # rho (roll & update)
        log_rho_window = torch.cat([log_rho_window[1:n], zeros_like.unsqueeze(0)], dim=0) + log_rho[i]

        # influence array update
        influence_window = torch.cat([influence_window[1:n], ones_like.unsqueeze(0)], dim=0) *(
                prev_not_done) +influence_reset *(1-prev_not_done)
        # print(influence_array)
        #rho gamma influence combined
        # rgi_array = gamma_array * rho_array* influence_array
        rgi_window = torch.exp(log_gamma_array+log_rho_window.clamp(clip_log_threshold_d,clip_log_threshold_u)) * influence_window


        #assign values from current reward (g_j += r_i * gamma^{i-j-1} * \Pi_{k=j}^{i-j-1} rho_k * (one if same episode and i-j<=n))
        g_value_targets[i_low:i+1] += rgi_window[w_low:] * reward_tp1[i]

        #Maybe add boostrapping if more than n steps have elapsed
        #(g_{i-n} += V(s_i) * gamma^{n} * \Pi_{k=i-n}^{n-1} rho_k * (one if same episode))
        if i>=(n-1):
            g_value_targets[i-n+1] += rgi_window[0]* not_done* gamma_value * values_tp1[i]

        #update cumulative importance weight of trajectory
        # cumulative_rhos[i] = previous_cum_rhos
        # previous_cum_rhos = (previous_cum_rhos * prev_not_done +ones_like * (1-prev_not_done)) * rho[i]
        cumulative_log_rhos[i] = previous_log_cum_rhos
        previous_log_cum_rhos = (previous_log_cum_rhos * prev_not_done +zeros_like * (1-prev_not_done)) + log_rho[i]



    # conditionally add boostrap value if episode had not ended on the n-1 last values
    # (g_{i-j} += V(s_i) * gamma^{i-j} * \Pi_{k=i-j}^{i-j-1} rho_k * (one if same episode))
    if n>1:
        g_value_targets[-n+1:] += rgi_window[1:] *not_done* gamma_value * values_tp1[-1]

    #build pg advantage
    if pg_mode == 'one_step':
        weighted_advantage = reward_tp1 + gamma_value * (1 - dones_tp1) * values_tp1  - values_t
        # policy_gradient_advs = weighted_advantage * cumulative_rhos
        policy_gradient_advs = weighted_advantage *  torch.exp(cumulative_log_rhos.clamp(clip_log_threshold_d,clip_log_threshold_u))
    elif pg_mode == 'n_step':
        weighted_advantage = g_value_targets - values_t
        # policy_gradient_advs = weighted_advantage * cumulative_rhos
        policy_gradient_advs = weighted_advantage * torch.exp(cumulative_log_rhos.clamp(clip_log_threshold_d,clip_log_threshold_u))
    elif pg_mode== 'simple_n':
        weighted_advantage = g_value_targets - values_t
        policy_gradient_advs = weighted_advantage
    elif pg_mode== 'short_n':
        weighted_advantage = g_value_targets - values_t
        policy_gradient_advs = weighted_advantage * torch.exp(log_rho)
    elif pg_mode== 'impala_pg':
        # weighted_advantage = g_value_targets - values_t
        weighted_advantage = reward_tp1 + gamma_value*(1-dones_tp1)* torch.cat([g_value_targets[1:], bootstrap_values.unsqueeze(0)], dim=0) - values_t
        # rewards + discounts * vs_t_plus_1 - values)
        policy_gradient_advs = weighted_advantage * torch.exp(log_rho)
    else:
        raise NotImplementedError



    return DetraceReturns(vs=g_value_targets.detach(),
                         pg_advantages=policy_gradient_advs.detach())



